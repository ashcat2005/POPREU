{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Forest for Star-Galaxy Classification using .fits\n",
    "This is nearly identical to stargalaxy, but is specific to the .fits data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows us to use PySpark in the iPython Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the path for spark installation\n",
    "# this is the path where you have built spark using sbt/sbt assembly\n",
    "os.environ['SPARK_HOME']=\"/Users/blorangest/Desktop/spark-1.3.1-bin-hadoop2.6\"\n",
    "# Append to PYTHONPATH so that pyspark could be found\n",
    "sys.path.append(\"/Users/blorangest/Desktop/spark-1.3.1-bin-hadoop2.6/python\")\n",
    "sys.path.append(os.path.join(os.environ['SPARK_HOME'], 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "sys.path.append(\"/Library/Python/2.7/site-packages\") #gives the location of pyfits and other python modules on local machine \n",
    "\n",
    "\n",
    "# Now we are ready to import Spark Modules\n",
    "try:\n",
    "    from pyspark.mllib.tree import RandomForest\n",
    "    from pyspark.mllib.tree import DecisionTreeModel\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark import SparkContext\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Error importing Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "import numpy as np\n",
    "import pyfits\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set some variables that will determine the properties of the random forest. test_size is the percentage of the data that will be used to test the model. num_trees is the number of trees in the forest. max_depth is the maximum depth of each tree. It must be no more than 30. k is the number of folds desired for kfolds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile = \"./round4_training_set.fits\"\n",
    "test_size = 0.2\n",
    "num_trees = 50\n",
    "max_depth = 8\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves a given RDD as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save (rdd, filename):\n",
    "    try:\n",
    "        shutil.rmtree(filename)\n",
    "    except Exception:\n",
    "        pass\n",
    "    rdd.saveAsTextFile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a slow way to get classification probabilities and number of trees that classify it as a star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs (model, data):\n",
    "    # Collect the individual decision trees as JavaArray objects\n",
    "    trees = model._java_model.trees()\n",
    "    ntrees = model.numTrees()\n",
    "    scores = DecisionTreeModel(trees[0]).predict(data.map(lambda x: x.features))\n",
    "\n",
    "    # For each tree, apply its prediction to the entire dataset and zip together the results\n",
    "    for i in range(1,ntrees):\n",
    "        dtm = DecisionTreeModel(trees[i])\n",
    "        scores = scores.zip(dtm.predict(data.map(lambda x: x.features)))\n",
    "        scores = scores.map(lambda x: x[0] + x[1])\n",
    "    \n",
    "    # Divide the accumulated scores over the number of trees\n",
    "    return scores.map(lambda x: x/ntrees), scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute test error by thresholding probabilistic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probTest(testData, model):\n",
    "    threshold = 0.5\n",
    "    probsAndScores = get_probs(model,testData)\n",
    "    probs = probsAndScores[0]\n",
    "    pred = probs.map(lambda x: 0 if x < threshold else 1)\n",
    "    lab_pred = testData.map(lambda lp: lp.label).zip(pred)\n",
    "    acc = lab_pred.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "    return (1 - acc), probsAndScores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the random forest classifier once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testOnce(data):\n",
    "    # split the data into training and testing sets\n",
    "    (trainingData, testData) = data.randomSplit([1-test_size, test_size])\n",
    "     # train the random forest\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},\n",
    "                                         numTrees=num_trees, featureSubsetStrategy=\"auto\",\n",
    "                                         impurity='gini', maxDepth = max_depth, maxBins=32)\n",
    "    # test the random forest\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "    Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())\n",
    "    Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())\n",
    "    Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())\n",
    "    Ns = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 1).count())\n",
    "    probsAndScores = probTest(testData, model)\n",
    "    threshold_accuracy = probsAndScores[0]\n",
    "    probs = probsAndScores[1].map(lambda x: x/num_trees)\n",
    "    labelsAndPredictions = labelsAndPredictions.zip(probs)\n",
    "    save(labelsAndPredictions, 'answers')\n",
    "    print ('Galaxy Purity = ' + str(Ng / (Ng+Ms)))\n",
    "    print ('Galaxy Completeness = ' + str(Ng / (Ng+Mg)))\n",
    "    print ('Star Purity = ' + str(Ns / (Ns+Mg)))\n",
    "    print ('Star Completeness = ' + str(Ns/(Ns+Ms)))\n",
    "    print ('Accuracy = ' + str(1 - testErr))\n",
    "    print ('Threshold method accuracy = ' + str(threshold_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs k folds cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfolds (data):\n",
    "    #folds = kFold(data, k) this would work in java\n",
    "    acc = 0\n",
    "    spurity = 0\n",
    "    scomp = 0\n",
    "    gpurity = 0\n",
    "    gcomp = 0\n",
    "    foldsize = data.count()/k\n",
    "    tested = sc.parallelize([])\n",
    "    for i in range(k):\n",
    "        test = sc.parallelize(data.subtract(tested).takeSample(False, foldsize))\n",
    "        tested = tested.union(test)\n",
    "        train = data.subtract(test)\n",
    "        # train the random forest\n",
    "        model = RandomForest.trainClassifier(train, numClasses=3, categoricalFeaturesInfo={},\n",
    "                                     numTrees=num_trees, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth = max_depth, maxBins=32)\n",
    "\n",
    "        predictions = model.predict(test.map(lambda x: x.features))\n",
    "        labelsAndPredictions = test.map(lambda lp: lp.label).zip(predictions)\n",
    "        testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(test.count())\n",
    "        Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())\n",
    "        Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())\n",
    "        Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())\n",
    "        Ns = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 1).count())\n",
    "        \n",
    "        gpurity += (Ng / (Ng+Ms))\n",
    "        gcomp += (Ng / (Ng+Mg))\n",
    "        spurity += (Ns / (Ns+Mg))\n",
    "        scomp += (Ns/(Ns+Ms))\n",
    "        acc += (1 - testErr)\n",
    "    \n",
    "    print 'with '+ str(k) + ' folds:'\n",
    "    print ('Average Galaxy Purity = ' + str(gpurity / k))\n",
    "    print ('Average Galaxy Completeness = ' + str(gcomp / k))\n",
    "    print ('Average Star Purity = ' + str(spurity / k))\n",
    "    print ('Average Star Completeness = ' + str(scomp / k))\n",
    "    print ('Average Accuracy = ' + str(acc / k))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets HDUlist of data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"stargalaxy\")\n",
    "x = pyfits.open(dataFile)\n",
    "headers = x[1].columns.names\n",
    "data = x[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets magnitudes from HDUlist and puts them in a matrix represented by nested lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "magHeads = ['MAG_AUTO_G','MAG_AUTO_R','MAG_AUTO_I','MAG_AUTO_Z','MAG_AUTO_Y']\n",
    "#magIdx = {m: m.index(m) for m in mag}\n",
    "mags = []\n",
    "for i in range(len(data)):\n",
    "    mags.append([])\n",
    "for m in magHeads:\n",
    "    x = data[m]\n",
    "    for i in range(len(data)):\n",
    "        mags[i].append(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the rest of the wanted features and puts them in a matrix represented by nested lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "want = ['WAVG_SPREAD_MODEL_G','WAVG_SPREAD_MODEL_R','WAVG_SPREAD_MODEL_I','WAVG_SPREAD_MODEL_Z','WAVG_SPREAD_MODEL_Y','A_IMAGE','B_IMAGE']\n",
    "rawdata = []\n",
    "labels = data['TRUE_CLASS'].tolist()\n",
    "for i in range(len(data)):\n",
    "    rawdata.append([])\n",
    "for w in want:\n",
    "    x = data[w]\n",
    "    for i in range(len(data)):\n",
    "        rawdata[i].append(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and add color to the nested list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    for j in range(len(magHeads)-1):\n",
    "         rawdata[i].append(mags[i][j]-mags[i][j+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data as a list of LabeledPoints and then reformat that list into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(rawdata)):\n",
    "    data.append(LabeledPoint(labels[i], rawdata[i]))\n",
    "data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galaxy Purity = 0.964799776949\n",
      "Galaxy Completeness = 0.996364814282\n",
      "Star Purity = 0.958708094849\n",
      "Star Completeness = 0.698956780924\n",
      "Accuracy = 0.964290301863\n",
      "Threshold method accuracy = 0.964290301863\n"
     ]
    }
   ],
   "source": [
    "testOnce(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
