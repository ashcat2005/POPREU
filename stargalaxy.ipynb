{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest for Star-Galaxy Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows us to use PySpark in the iPython Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the path for spark installation\n",
    "# this is the path where you have built spark using sbt/sbt assembly\n",
    "os.environ['SPARK_HOME']=\"/Users/blorangest/Desktop/spark-1.3.1-bin-hadoop2.6\"\n",
    "\n",
    "# Append to PYTHONPATH so that pyspark could be found\n",
    "sys.path.append(\"/Users/blorangest/Desktop/spark-1.3.1-bin-hadoop2.6/python\")\n",
    "sys.path.append(os.path.join(os.environ['SPARK_HOME'], 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "# Now we are ready to import Spark Modules\n",
    "try:\n",
    "    from pyspark.mllib.tree import RandomForest\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark import SparkContext\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Error importing Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set some variables that will determine the properties of the random forest. \n",
    "test_size is the percentage of the data that will be used to test the model.\n",
    "num_trees is the number of trees in the forest.\n",
    "max_depth is the maximum depth of each tree. It must be no more than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile = \"cfhtlens_matched.csv\"\n",
    "test_size = 0.2\n",
    "num_trees = 6\n",
    "max_depth = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used to add colors to the feature data by taking the differences of adjacent magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addColors (features):\n",
    "    for i in range (len(features)-1):\n",
    "        features.append(features[i+1]-features[i])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does data preprocessing. It removes unwanted columns and puts the relevant data in LabeledPoint objects. Note that, in this particular dataset, one of the columns has comma seperated values enclosed by quotes that all belong under a single heading. This column will be quotes[1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse (line): \n",
    "    quotes = np.array([x for x in line.split('\"')])\n",
    "    row = quotes[0].split(',')[:-1] + [quotes[1]] + quotes[2].split(',')[1:]\n",
    "    label = float(row[heads['true_class']])\n",
    "    want = ['MAG_u', 'MAG_g', 'MAG_r', 'MAG_i', 'MAG_z']\n",
    "    want_index = []\n",
    "    for w in want:\n",
    "        want_index.append(heads[w])\n",
    "    features = []\n",
    "    for i in range (len(row)):\n",
    "        for w in want_index:\n",
    "            if i == w:\n",
    "                features.append(float(row[i]))\n",
    "    features = addColors(features)\n",
    "    return LabeledPoint(label, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the SparkContext and load the raw data into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"stargalaxy\")\n",
    "rawData = sc.textFile(dataFile) # is an RDD with 66389 things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove the header row and use the parse function above to map the data into a new RDD that will be usable by the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = rawData.first()\n",
    "lines = rawData.filter(lambda x: x != header) #now the header is gone\n",
    "header_split = str(header).split(',')\n",
    "heads = {}\n",
    "for i in range( len(header_split)):\n",
    "    heads[header_split[i]] = i\n",
    "data = lines.map(parse).cache() # RDD of LabeledPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([1-test_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=num_trees, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth = max_depth, maxBins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the random forest model and print some metrics to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity = 0.971548188653\n",
      "Completeness = 0.987837720441\n",
      "1 - Accuracy = 0.0356927256263\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "Mg = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 1).count())\n",
    "Ng = float(labelsAndPredictions.filter(lambda (v, p): v == 0 and p == 0).count())\n",
    "Ms = float(labelsAndPredictions.filter(lambda (v, p): v == 1 and p == 0).count())\n",
    "print ('Purity = ' + str(Ng / (Ng+Ms)))\n",
    "print ('Completeness = ' + str(Ng / (Ng+Mg)))\n",
    "print ('1 - Accuracy = ' + str(testErr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
